<html>
<head>
<title>Rahul Kidambi</title>
</head>
<body bgcolor=#f2f2f2 text=black link=blue vlink=#990000 style='font-family:sans-serif'>


<table width="1100" align=center cellspacing="15">
	<tr>
		<td width="12%">
			<IMG src="RK-Mar2017.jpg" width="150">
		</td>
		<td width="88%">
			<div align="left">
			<h1> Rahul Kidambi </h1><font face = "Bedrock">
			<p> I earned my PhD studying ML from UW Seattle (Advisor: <a href="https://homes.cs.washington.edu/~sham/" target="_blank"> Sham M. Kakade</a>) and then pursued post-doctoral research at Cornell University.
			</p>

			<p> <strong>contact</strong>: rkidambi AT uw DOT edu  | <a href="https://scholar.google.com/citations?hl=en&user=vSxL7K8AAAAJ" target="_blank">Google Scholar</a>.</p></font>
			</div>
		</td>
	</tr>
</table>
<table width="1100" align=center cellspacing="20">
	<tr>
		<td width="100%">
		<hr>
			<h2> Research</h2><font face = "Bedrock">
			<p>I study topics in Machine Learning and AI with a particular emphasis on:</p>

			<p> <li type="square">Reinforcement Learning & Contextual Bandits (and their intersection with imitation learning, representation learning and counterfactual (offline) estimation).
			<p> <li type="square">Stochastic Approximation methods (for designing improved algorithms for large-scale Machine Learning and Deep Learning problems).</li></p><br>

			I consider issues in these topics with applications to Human-facing systems including recommendation systems and robotics.</font>
		</td>
	</tr>

	<tr>
		<td width = "100%">
		<hr>

		<h2> PhD Thesis</h2><font face = "Bedrock">
			<strong>Stochastic Gradient Descent For Modern Machine Learning: Theory, Algorithms And Applications,</strong><br>
			Rahul Kidambi. <br>
			PhD Thesis, University of Washington Seattle, June 2019. <br>
			[<a href="https://digital.lib.washington.edu/researchworks/handle/1773/44183" target="_blank">Link</a>]</font><br><br>
		<hr>

		<h2> Publications</h2><font face = "Bedrock">
			Asterisk [*] indicates alphabetical ordering of authors.</font><br><br>

			<h3> Pre-Prints</h3><font face = "Bedrock">

			<li type="square"> <strong>Optimism is all you need: Model-based Imitation Learning from Observation Alone,</strong><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Jonathan Chang, Wen Sun.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/2102.10769" target="_blank">abs/2102.10769</a>, February 2021. <br>
				Also appears in Workshop on Self-Supervised Learning for Reinforcement Learning at <strong>ICLR</strong> 2021.<br>
				<br>

			<li type="square"> <strong>Top-k eXtreme Contextual Bandits with Arm Hierarchy,</strong><br>
				Rajat Sen, Alexander Rakhlin, Lexing Ying, <font color=#ab1f1f>Rahul Kidambi</font>, Dean Foster, Daniel Hill, Inderjit Dhillon.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/2102.07800" target="_blank">abs/2102.07800</a>, February 2021. <br>
				<br>

			<li type="square"> <strong>Making Paper Reviewing Robust to Bid Manipulation Attacks,</strong><br>
				Ruihan Wu, Chuan Guo, Felix Wu, <font color=#ab1f1f>Rahul Kidambi</font>, Laurens van der Maaten, Kilian Q. Weinberger.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/2102.06020" target="_blank">abs/2102.06020</a>, February 2021. <br>
				<br></font>

		<h3> Published Papers</h3><font face = "Bedrock">

			<li type="square"> <strong>MOReL: Model-Based Offline Reinforcement Learning,</strong><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Aravind Rajeswaran, Praneeth Netrapalli, Thorsten Joachims.<br>
				In Proc. <i>Neural Information Processing Systems</i> <strong>(NeurIPS)</strong>, 2020.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/2005.05951" target="_blank">abs/2005.05951</a>, May 2020. <br>
				[<a href="https://sites.google.com/view/morel" target="_blank">Project Page</a>]<br>
				<br>

			<li type="square"> <strong>Leverage Score Sampling for Faster Accelerated Regression and ERM,</strong> [*]<br>
				Naman Agarwal, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Yin Tat Lee, Praneeth Netrapalli, Aaron Sidford.<br>
				In Proc. <i>Algorithmic Learning Theory</i> <strong>(ALT)</strong>, 2020.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1711.08426" target="_blank">abs/1711.08426</a>, November 2017. <br>
				<br>


			<li type="square"> <strong>The Step Decay Schedule: A Near Optimal Geometrically Decaying Learning Rate Procedure For Least Squares,</strong> [*] <sup><a href="#fn3" id="ref3">3</a></sup><br>
				Rong Ge, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Praneeth Netrapalli.<br>
				In Proc. <i>Neural Information Processing Systems</i> <strong>(NeurIPS)</strong>, 2019.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1904.12838" target="_blank">abs/1904.12838</a>, April 2019.<br>
				[<a href="http://praneethnetrapalli.org/FinalIterate-StepDecay.pdf" target="_blank">Slides</a>]<br>
				<br>


			<li type="square"> <strong>On the insufficiency of existing Momentum schemes for Stochastic Optimization,</strong><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Praneeth Netrapalli, Prateek Jain, Sham M. Kakade.<br>
				In <i>International Conference on Learning Representations</i> <strong>(ICLR)</strong>, 2018. (<font color="red">Oral Presentation:</font> 23/1002 submissions &asymp; 2% Acceptance Rate.)<br>
				Also an <i>invited</i> paper at <i>Information Theory and Applications</i> <strong>(ITA)</strong> workshop, San Diego, 2018.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1803.05591" target="_blank">abs/1803.05591</a>, March 2018.<br>
				[<a href="https://openreview.net/forum?id=rJTutzbA-" target="_blank">Open Review</a>] [<a href="https://ieeexplore.ieee.org/document/8503173" target="_blank">ITA version</a>] [<a href="https://github.com/rahulkidambi/AccSGD" target="_blank">Code</a>] <br>
				<br>


			<li type="square"> <strong>Accelerating Stochastic Gradient Descent for least squares regression</strong><sup><a href="#fn2" id="ref2">2</a></sup>, [*]<br>
				Prateek Jain, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Praneeth Netrapalli, Aaron Sidford.<br>
				In Proc.<i> Conference on Learning Theory</i> <strong>(COLT)</strong>, 2018.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1704.08227" target="_blank">abs/1704.08227</a>, April 2017.<br>
				[<a href="http://proceedings.mlr.press/v75/jain18a.html" target="_blank">COLT proceedings</a>] [<a href="https://www.youtube.com/watch?v=_UFGB2MBo4o" target="_blank">Video</a> (Sham at MSR)]<br>
				<br>

			<li type="square"> <strong>Parallelizing Stochastic Gradient Descent for Least Squares Regression: mini-batching, averaging, and model misspecification</strong><sup><a href="#fn1" id="ref1">1</a></sup>, [*]<br>
				Prateek Jain, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Praneeth Netrapalli, Aaron Sidford.<br>
				In <i>Journal of Machine Learning Research</i> <strong>(JMLR)</strong>, Vol. 18 (223), July 2018.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1610.03774" target="_blank">abs/1610.03774</a>, October 2016. Updated, April 2018.<br>
				[<a href="http://jmlr.org/papers/v18/16-595.html" target="_blank">JMLR link</a>]<br>
				<br>

			<li type="square"> <strong>Submodular Hamming Metrics</strong>,<br>
				Jennifer Gillenwater, Rishabh K. Iyer, Bethany Lusch, <font color=#ab1f1f>Rahul Kidambi</font>, Jeff A. Bilmes.<br>
				In Proc. <i>Neural Information Processing Systems</i> <strong>(NeurIPS)</strong>, December 2015. (<font color="red">Spotlight Presentation</font>)<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1511.02163" target="_blank">abs/1511.02163</a>, November 2015.<br>
				[<a href="https://papers.nips.cc/paper/5741-submodular-hamming-metrics" target="_blank">NeurIPS proceedings</a>]<br>
				<br>


			<li type="square"> <strong>Deformable trellises on factor graphs for robust microtubule tracking in clutter,</strong><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Min-Chi Shih, Kenneth Rose.<br>
				In Proc. <i>International Symposium on Biomedical Imaging</i> <strong>(ISBI)</strong>, May 2012.<br>
				[<a href="https://ieeexplore.ieee.org/document/6235638" target="_blank">ISBI proceedings</a>]<br>
				<br>

			<h3> Invited/Workshop Papers, Open Problems</h3>

			<li type="square"> <strong>Open Problem: Do Good Algorithms Necessarily Query Bad Points?,</strong> [*]<br>
				Rong Ge, Prateek Jain, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Dheeraj M. Nagaraj, Praneeth Netrapalli.<br>
				In Proc. <i>Conference on Learning Theory</i> <strong>(COLT)</strong>, 2019.<br>
				[<a href="http://proceedings.mlr.press/v99/ge19b.html" target="_blank">COLT Proceedings</a>] <br>
				<br>

			<li type="square"> <strong>A Markov Chain Theory Approach to Characterizing the Minimax Optimality of Stochastic Gradient Descent (for Least Squares),</strong> [*]<br>
				Prateek Jain, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Praneeth Netrapalli, Venkata Krishna Pillutla, Aaron Sidford.<br>
				<i>Invited</i> paper at <strong>FSTTCS</strong> 2017.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1710.09430" target="_blank">abs/1710.09430</a>, October 2017.<br>
				<br>


			<li type="square"> <strong>On Shannon capacity and causal estimation,</strong><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Sreeram Kannan.<br>
				<i>Invited</i> paper at <strong>Allerton Conference on Communication, Control, and Computing</strong>, 2015.<br>
				[<a href="https://ieeexplore.ieee.org/document/7447115" target="_blank">Allerton proceedings</a>]<br>
				<br></font>

			<h3> Technical Reports</h3><font face = "Bedrock">

			<li type="square"> <strong>Efficient Estimation of Generalization Error and Bias-Variance Components of Ensembles,</strong><br>
				Dhruv Mahajan, Vivek Gupta, S. Sathiya Keerthi, Sundararajan Sellamanickam, Shravan Narayanamurthy, <font color=#ab1f1f>Rahul Kidambi</font>.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1711.05482" target="_blank">abs/1711.05482</a>, November 2017.<br>
				<br>

			<li type="square"> <strong>A Structured Prediction Approach for Missing Value Imputation,</strong><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Vinod Nair, Sundararajan Sellamanickam, S. Sathiya Keerthi.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1311.2137" target="_blank">abs/1311.2137</a>, November 2013.<br>
				<br>

			<li type="square"> <strong>A Quantitative Evaluation Framework for Missing Value Imputation Algorithms,</strong><br>
				Vinod Nair, <font color=#ab1f1f>Rahul Kidambi</font>, Sundararajan Sellamanickam, S. Sathiya Keerthi, Johannes Gehrke, Vijay Narayanan.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1311.2276" target="_blank">abs/1311.2276</a>, November 2013.<br>
				<br>



			The <a href="http://dblp.uni-trier.de/pers/hd/k/Kidambi:Rahul" target="_blank">dblp</a> maintains a listing of my papers.<br><br><br>

			<sup id="fn1">1. Earlier Version Titled "<strong>Parallelizing Stochastic Approximation Through Mini-Batching and Tail Averaging.</strong>"<a href="#ref1" title="Jump back to footnote 1 in the text.">&#8617</a></sup><br>
			<sup id="fn2">2. Earlier Version Titled "<strong>Accelerating Stochastic Gradient Descent.</strong>"<a href="#ref2" title="Jump back to footnote 2 in the text.">&#8617</a></sup><br>
			<sup id="fn3">3. Earlier Version Titled "<strong>The Step Decay Schedule: A Near Optimal Geometrically Decaying Learning Rate Procedure.</strong>"<a href="#ref3" title="Jump back to footnote 3 in the text.">&#8617</a></sup></font>
		</td>
	</tr>

<tr>
<td width = "100%">
<hr>
<h2> Academic Service</h2><font face = "Bedrock">
<li type="square"> <strong>Conference Reviewing</strong>: COLT, NeurIPS, ICML, ICLR, AISTATS, ALT, ISIT. </li><br>
<li type="square"> <strong>Journal Refereeing</strong>: Journal of Machine Learning Research (JMLR), Electronic Journal of Statistics, IEEE Transactions of Information Theory.</li><br>
<br>
I am also a member of the JMLR editorial board.</font>
</td>
</tr>

<tr>
<td width = "100%">
<hr>
<h2> Teaching</h2><font face = "Bedrock">
Some classes I have TA'ed for include:<br><br>
<li type="square"><a href="https://courses.cs.washington.edu/courses/cse547/18sp/" target="_blank">CSE 547/STAT 548</a>: Machine Learning for Big Data. (Spring 2018).<br>
<li type="square">EE 514a: Information Theory-I (Autumn 2015).<br>
<br>
<hr></font>
</td>
</tr>

</body>

</html>
