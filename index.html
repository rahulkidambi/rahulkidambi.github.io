<html>
<head>
<title>Rahul Kidambi</title>
</head>
<body bgcolor=#f2f2f2 text=black link=blue vlink=#990000 style='font-family:sans-serif'>


<table width="1100" align=center cellspacing="15">
	<tr>
		<td width="12%">
			<IMG src="RK-Mar2017.jpg" width="150">
		</td>
		<td width="88%">
			<div align="left">
			<h1> Rahul Kidambi </h1><font face = "Bedrock">
			<p> I earned my PhD studying ML from University of Washington Seattle (Advisor: <a href="https://homes.cs.washington.edu/~sham/" target="_blank"> Sham M. Kakade</a>).</p>
			I then pursued post-doctoral research at Cornell University. </p>

			<p> <strong>contact</strong>: rkidambi AT uw DOT edu  | <a href="https://scholar.google.com/citations?hl=en&user=vSxL7K8AAAAJ" target="_blank">Google Scholar</a>.</p></font>
			</div>
		</td>
	</tr>
</table>
<table width="1100" align=center cellspacing="20">
	<tr>
		<td width="100%">
		<hr>
			<h2> Research</h2><font face = "Bedrock">
			<p>I am interested in Machine Learning, Deep Learning and AI, with an emphasis on:</p>

			<p> <li type="square">Reinforcement Learning including topics in exploration, offline RL, counterfactual reasoning and learning with expert supervision (imitation learning).
			<p> <li type="square">Stochastic Gradient Methods/Stochastic Approximation for large-scale Machine Learning and Deep Learning.</li></p><br>

			I consider issues in these topics with applications to Human-facing systems including recommendation systems and robotics.</font><br><br>

			<hr>

			<h3>Research Threads</h3><font face = "Bedrock">

			<h4>Stochastic Gradient Descent for large scale ML</h4>
			<li type="square"> Mini-Batch/parallel/distributed SGD [<a href="https://arxiv.org/abs/1610.03774" target="_blank">JMLR 2018</a>]<br>
			<li type="square"> Accelerated/Momentum based SGD [<a href="https://arxiv.org/abs/1704.08227" target="_blank">COLT 2018</a>, <a href="https://arxiv.org/abs/1803.05591" target="_blank">ICLR 2018</a>]<br>
			<li type="square"> Learning Rate Scheduling for SGD [<a href="https://arxiv.org/abs/1904.12838" target="_blank">NeurIPS 2019</a>]<br><br>

			<h4>Algorithmic Frameworks for Model-Based Interactive Learning</h4>
			<li type="square"> MOReL - Offline RL [<a href="https://arxiv.org/abs/2005.05951" target="_blank">NeurIPS 2020</a>]<br>
			<li type="square"> MobILE - Third Person Imitation Learning [<a href="https://arxiv.org/abs/2102.10769" target="_blank">NeurIPS 2021</a> (to appear)]<br>
			<li type="square"> MILO - Imitation Learning with offline behavior data [<a href="https://arxiv.org/abs/2106.03207" target="_blank">NeurIPS 2021</a> (to appear)]<br></font>

		</td>
	</tr>

	<tr>
		<td width = "100%">
		<hr>

		<h2> PhD Thesis</h2><font face = "Bedrock">
			<strong>Stochastic Gradient Descent For Modern Machine Learning: Theory, Algorithms And Applications,</strong><br>
			Rahul Kidambi. <br>
			PhD Thesis, University of Washington Seattle, June 2019. <br>
			[<a href="https://digital.lib.washington.edu/researchworks/handle/1773/44183" target="_blank">Link</a>]</font><br><br>
		<hr>

		<h2> Publications</h2><font face = "Bedrock">
			Asterisk [*] indicates alphabetical ordering of authors.</font><br><br>

		<h3> Conference/Journal Papers</h3><font face = "Bedrock">


			<li type="square"> <strong>Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage,</strong><br>
				Jonathan Chang, Masatoshi Uehara, Dhruv Sreenivas, <font color=#ab1f1f>Rahul Kidambi</font>, Wen Sun.<br>
				To Appear, <i>Neural Information Processing Systems</i> <strong>(NeurIPS)</strong>, 2021.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/2106.03207" target="_blank">abs/2106.03207</a>, May 2021. <br>
				[<a href="https://github.com/jdchang1/milo" target="_blank">Code</a>]<br>
				<br>

			<li type="square"> <strong>MobILE: Model-based Imitation Learning from Observation Alone,</strong><sup><a href="#fn4" id="ref4">4</a></sup><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Jonathan Chang, Wen Sun.<br>
				To Appear, <i>Neural Information Processing Systems</i> <strong>(NeurIPS)</strong>, 2021.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/2102.10769" target="_blank">abs/2102.10769</a>, February 2021. <br>
				<br>

			<li type="square"> <strong>Top-k eXtreme Contextual Bandits with Arm Hierarchy,</strong><br>
				Rajat Sen, Alexander Rakhlin, Lexing Ying, <font color=#ab1f1f>Rahul Kidambi</font>, Dean Foster, Daniel Hill, Inderjit Dhillon.<br>
				In Proc. <i>International Conference on Machine Learning</i> <strong>(ICML)</strong>, 2021.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/2102.07800" target="_blank">abs/2102.07800</a>, February 2021. <br>
				<br>

			<li type="square"> <strong>Making Paper Reviewing Robust to Bid Manipulation Attacks,</strong><br>
				Ruihan Wu, Chuan Guo, Felix Wu, <font color=#ab1f1f>Rahul Kidambi</font>, Laurens van der Maaten, Kilian Q. Weinberger.<br>
				In Proc. <i>International Conference on Machine Learning</i> <strong>(ICML)</strong>, 2021.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/2102.06020" target="_blank">abs/2102.06020</a>, February 2021. <br>
				<br>

			<li type="square"> <strong>MOReL: Model-Based Offline Reinforcement Learning,</strong><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Aravind Rajeswaran, Praneeth Netrapalli, Thorsten Joachims.<br>
				In Proc. <i>Neural Information Processing Systems</i> <strong>(NeurIPS)</strong>, 2020.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/2005.05951" target="_blank">abs/2005.05951</a>, May 2020. <br>
				[<a href="https://sites.google.com/view/morel" target="_blank">Project Page</a>]<br>
				<br>

			<li type="square"> <strong>Leverage Score Sampling for Faster Accelerated Regression and ERM,</strong> [*]<br>
				Naman Agarwal, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Yin Tat Lee, Praneeth Netrapalli, Aaron Sidford.<br>
				In Proc. <i>Algorithmic Learning Theory</i> <strong>(ALT)</strong>, 2020.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1711.08426" target="_blank">abs/1711.08426</a>, November 2017. <br>
				<br>


			<li type="square"> <strong>The Step Decay Schedule: A Near Optimal Geometrically Decaying Learning Rate Procedure For Least Squares,</strong> [*] <sup><a href="#fn3" id="ref3">3</a></sup><br>
				Rong Ge, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Praneeth Netrapalli.<br>
				In Proc. <i>Neural Information Processing Systems</i> <strong>(NeurIPS)</strong>, 2019.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1904.12838" target="_blank">abs/1904.12838</a>, April 2019.<br>
				[<a href="http://praneethnetrapalli.org/FinalIterate-StepDecay.pdf" target="_blank">Slides</a>]<br>
				<br>

		  <li type="square"> <strong>Open Problem: Do Good Algorithms Necessarily Query Bad Points?,</strong> [*]<br>
					Rong Ge, Prateek Jain, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Dheeraj M. Nagaraj, Praneeth Netrapalli.<br>
					In Proc. <i>Conference on Learning Theory</i> <strong>(COLT)</strong>, 2019.<br>
					[<a href="http://proceedings.mlr.press/v99/ge19b.html" target="_blank">COLT Proceedings</a>] <br>
					<br>

			<li type="square"> <strong>On the insufficiency of existing Momentum schemes for Stochastic Optimization,</strong><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Praneeth Netrapalli, Prateek Jain, Sham M. Kakade.<br>
				In <i>International Conference on Learning Representations</i> <strong>(ICLR)</strong>, 2018. (<font color="red">Oral Presentation:</font> 23/1002 submissions &asymp; 2% Acceptance Rate.)<br>
				Also an <i>invited</i> paper at <i>Information Theory and Applications</i> <strong>(ITA)</strong> workshop, San Diego, 2018.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1803.05591" target="_blank">abs/1803.05591</a>, March 2018.<br>
				[<a href="https://openreview.net/forum?id=rJTutzbA-" target="_blank">Open Review</a>] [<a href="https://ieeexplore.ieee.org/document/8503173" target="_blank">ITA version</a>] [<a href="https://github.com/rahulkidambi/AccSGD" target="_blank">Code</a>] <br>
				<br>


			<li type="square"> <strong>Accelerating Stochastic Gradient Descent for least squares regression</strong><sup><a href="#fn2" id="ref2">2</a></sup>, [*]<br>
				Prateek Jain, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Praneeth Netrapalli, Aaron Sidford.<br>
				In Proc.<i> Conference on Learning Theory</i> <strong>(COLT)</strong>, 2018.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1704.08227" target="_blank">abs/1704.08227</a>, April 2017.<br>
				[<a href="http://proceedings.mlr.press/v75/jain18a.html" target="_blank">COLT proceedings</a>] [<a href="https://www.youtube.com/watch?v=_UFGB2MBo4o" target="_blank">Video</a> (Sham at MSR)]<br>
				<br>

			<li type="square"> <strong>Parallelizing Stochastic Gradient Descent for Least Squares Regression: mini-batching, averaging, and model misspecification</strong><sup><a href="#fn1" id="ref1">1</a></sup>, [*]<br>
				Prateek Jain, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Praneeth Netrapalli, Aaron Sidford.<br>
				In <i>Journal of Machine Learning Research</i> <strong>(JMLR)</strong>, Vol. 18 (223), July 2018.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1610.03774" target="_blank">abs/1610.03774</a>, October 2016. Updated, April 2018.<br>
				[<a href="http://jmlr.org/papers/v18/16-595.html" target="_blank">JMLR link</a>]<br>
				<br>

			<li type="square"> <strong>Submodular Hamming Metrics</strong>,<br>
				Jennifer Gillenwater, Rishabh K. Iyer, Bethany Lusch, <font color=#ab1f1f>Rahul Kidambi</font>, Jeff A. Bilmes.<br>
				In Proc. <i>Neural Information Processing Systems</i> <strong>(NeurIPS)</strong>, December 2015. (<font color="red">Spotlight Presentation</font>)<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1511.02163" target="_blank">abs/1511.02163</a>, November 2015.<br>
				[<a href="https://papers.nips.cc/paper/5741-submodular-hamming-metrics" target="_blank">NeurIPS proceedings</a>]<br>
				<br>


			<li type="square"> <strong>Deformable trellises on factor graphs for robust microtubule tracking in clutter,</strong><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Min-Chi Shih, Kenneth Rose.<br>
				In Proc. <i>International Symposium on Biomedical Imaging</i> <strong>(ISBI)</strong>, May 2012.<br>
				[<a href="https://ieeexplore.ieee.org/document/6235638" target="_blank">ISBI proceedings</a>]<br>
				<br>

			<h3> Invited/Workshop Papers</h3>

			<li type="square"> <strong>A Markov Chain Theory Approach to Characterizing the Minimax Optimality of Stochastic Gradient Descent (for Least Squares),</strong> [*]<br>
				Prateek Jain, Sham M. Kakade, <font color=#ab1f1f>Rahul Kidambi</font>, Praneeth Netrapalli, Venkata Krishna Pillutla, Aaron Sidford.<br>
				<i>Invited</i> paper at <strong>FSTTCS</strong> 2017.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1710.09430" target="_blank">abs/1710.09430</a>, October 2017.<br>
				<br>


			<li type="square"> <strong>On Shannon capacity and causal estimation,</strong><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Sreeram Kannan.<br>
				<i>Invited</i> paper at <strong>Allerton Conference on Communication, Control, and Computing</strong>, 2015.<br>
				[<a href="https://ieeexplore.ieee.org/document/7447115" target="_blank">Allerton proceedings</a>]<br>
				<br></font>

			<h3> Technical Reports</h3><font face = "Bedrock">

			<li type="square"> <strong>Efficient Estimation of Generalization Error and Bias-Variance Components of Ensembles,</strong><br>
				Dhruv Mahajan, Vivek Gupta, S. Sathiya Keerthi, Sundararajan Sellamanickam, Shravan Narayanamurthy, <font color=#ab1f1f>Rahul Kidambi</font>.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1711.05482" target="_blank">abs/1711.05482</a>, November 2017.<br>
				<br>

			<li type="square"> <strong>A Structured Prediction Approach for Missing Value Imputation,</strong><br>
				<font color=#ab1f1f>Rahul Kidambi</font>, Vinod Nair, Sundararajan Sellamanickam, S. Sathiya Keerthi.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1311.2137" target="_blank">abs/1311.2137</a>, November 2013.<br>
				<br>

			<li type="square"> <strong>A Quantitative Evaluation Framework for Missing Value Imputation Algorithms,</strong><br>
				Vinod Nair, <font color=#ab1f1f>Rahul Kidambi</font>, Sundararajan Sellamanickam, S. Sathiya Keerthi, Johannes Gehrke, Vijay Narayanan.<br>
				ArXiv manuscript, <a href="https://arxiv.org/abs/1311.2276" target="_blank">abs/1311.2276</a>, November 2013.<br>
				<br>



			The <a href="http://dblp.uni-trier.de/pers/hd/k/Kidambi:Rahul" target="_blank">dblp</a> maintains a listing of my papers.<br><br><br>

			<sup id="fn1">1. Earlier Version Titled "<strong>Parallelizing Stochastic Approximation Through Mini-Batching and Tail Averaging.</strong>"<a href="#ref1" title="Jump back to footnote 1 in the text.">&#8617</a></sup><br>
			<sup id="fn2">2. Earlier Version Titled "<strong>Accelerating Stochastic Gradient Descent.</strong>"<a href="#ref2" title="Jump back to footnote 2 in the text.">&#8617</a></sup><br>
			<sup id="fn3">3. Earlier Version Titled "<strong>The Step Decay Schedule: A Near Optimal Geometrically Decaying Learning Rate Procedure.</strong>"<a href="#ref3" title="Jump back to footnote 3 in the text.">&#8617</a></sup><br>
			<sup id="fn4">4. Earlier Version Titled "<strong>Optimism is all you need: Model-based Imitation Learning from Observation Alone.</strong>"<a href="#ref4" title="Jump back to footnote 4 in the text.">&#8617</a></sup></font>
		</td>
	</tr>

<tr>
<td width = "100%">
<hr>
<h2> Academic Service</h2><font face = "Bedrock">
<li type="square"> <strong>Conference Reviewing</strong>: COLT, NeurIPS, ICML, ICLR, AISTATS, ALT, ISIT. </li><br>
<li type="square"> <strong>Journal Refereeing</strong>: Journal of Machine Learning Research (JMLR), Electronic Journal of Statistics, IEEE Transactions of Information Theory, IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI).</li><br>
<br>
I am also a member of the JMLR editorial board.</font>
</td>
</tr>

<tr>
<td width = "100%">
<hr>
<h2> Teaching</h2><font face = "Bedrock">
Some classes I have TA'ed for include:<br><br>
<li type="square"><a href="https://courses.cs.washington.edu/courses/cse547/18sp/" target="_blank">CSE 547/STAT 548</a>: Machine Learning for Big Data. (Spring 2018).<br>
<li type="square">EE 514a: Information Theory-I (Autumn 2015).<br>
<br>
<hr></font>
</td>
</tr>

</body>

</html>
