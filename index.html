<html> 
<head> 
<title>
Rahul Kidambi
</title> 
</head> 
<body bgcolor=#f2f2f2 text=black link=blue vlink=#990000 style='font-family:Arial'>
<IMG src="RK-blossom2017.jpg" HR WIDTH="17%" HR HEIGHT="44%" ALIGN="left" HSPACE="15">
<h1> Rahul Kidambi </h1>

<p> I am a graduate student of <a href="https://homes.cs.washington.edu/~sham/" target="_blank"> Prof. Sham M. Kakade</a>  studying Machine Learning at the University of Washington, Seattle. </p>
<p> Contact: <strong>rkidambi AT uw DOT edu</strong>.</p>
<hr>

<h2> Research: </h2>
<p> My research centers around the design and analysis of scalable Algorithms for Machine Learning, as viewed through the lens of Optimization and Statistics.</p> 

<p> I hold an active interest in the practical aspects of deep learning and non-convex optimization.</p>

<p> Previously, I worked on problems at the intersection of Structured Prediction, Semi-Supervised Learning and Active Learning. </p>
<hr>

<h2> Recent Papers/Preprints: </h2>

<li> 
<strong>  On the insufficiency of existing Momentum schemes for Stochastic Optimization,</strong><br>
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain and Sham M. Kakade.<br>
To Appear, International Conference on Learning Representations (ICLR), 2018. (<font color="red">Selected for Oral Presentation; 2% Acceptance Rate</font>)<br>
Link to the manuscript/reviews: <a href="https://openreview.net/forum?id=rJTutzbA-" target="_blank">Open Review</a>, January 2018.<br>
Code for Accelerated SGD: <a href="https://github.com/rahulkidambi/AccSGD" target="_blank">Git Repo</a>.<br>
<br>

<li> 
<strong>  Leverage Score Sampling for Faster Accelerated Regression and ERM,</strong><br>
(&alpha;-&beta; order) with Naman Agarwal, Sham M. Kakade, Yin Tat Lee, Praneeth Netrapalli and Aaron Sidford.<br>
ArXiv manuscript, <a href="https://arxiv.org/abs/1711.08426" target="_blank">abs/1711.08426</a>, November 2017.<br>
<br>

<li> 
<strong> A Markov Chain Theory Approach to Characterizing the Minimax Optimality of Stochastic Gradient Descent (for Least Squares),</strong><br>
(&alpha;-&beta; order) with Prateek Jain, Sham M. Kakade, Praneeth Netrapalli, Venkata Krishna Pillutla and Aaron Sidford.<br>
ArXiv manuscript, <a href="https://arxiv.org/abs/1710.09430" target="_blank">abs/1710.09430</a>, October 2017.<br>
<i>Invited</i> paper at FSTTCS 2017.<br>
<br>

<li> 
<strong> Accelerating Stochastic Gradient Descent,</strong><br>
(&alpha;-&beta; order) with Prateek Jain, Sham M. Kakade, Praneeth Netrapalli and Aaron Sidford.<br>
ArXiv manuscript, <a href="https://arxiv.org/abs/1704.08227" target="_blank">abs/1704.08227</a>, April 2017.<br>
Talk video: Sham at <a href="https://www.microsoft.com/en-us/research/video/accelerating-stochastic-gradient-descent/#" target="_blank">MSR</a>.<br>
<br>

<li> 
<strong> Parallelizing Stochastic Approximation Through Mini-Batching and Tail Averaging,</strong><br>
(&alpha;-&beta; order) with Prateek Jain, Sham M. Kakade, Praneeth Netrapalli and Aaron Sidford.<br>
ArXiv manuscript, <a href="https://arxiv.org/abs/1610.03774" target="_blank">abs/1610.03774</a>, October 2016.<br>
Accepted for journal publication pending minor revision, March 2017.<br><br>

The <a href="http://dblp.uni-trier.de/pers/hd/k/Kidambi:Rahul" target="_blank">dblp</a> listing provides a complete set of my papers.

<hr>

<h2> Academic Service: </h2>
<li type="square"> Conference Reviewing: ISMB 2012, NIPS 2016, COLT 2017.<br>
<li type="square"> Journal Reviewing: Journal of Machine Learning Research (JMLR) - 2015, Electronic Journal of Statistics (EJS) - 2017.<br>
<br>
<hr>

<h2> Teaching: </h2>
<li type="square"> EE 514a: Information Theory-I (Autumn 2015).<br>
<li type="square"> EE 215: Fundamentals of Electrical Engineering (Autumn 2014, Winter 2015).<br>
<br>
<hr>

<h2> Contact Information: </h2>
<address>Rahul Kidambi,<br>
Department of Electrical Engineering,<br>
185 Stevens Way, AE100R Campus Box 352500,<br>
University of Washington,<br>
Seattle, WA 98195-2500, USA.<br></address>
</body> 
</html> 
